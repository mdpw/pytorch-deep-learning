{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumani\n",
    "# 29-7-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network for MNIST Classification using PyTorch\n",
    "\n",
    "In this notebook, we will walk through the process of building, training, and evaluating a fully connected neural network for classifying handwritten digits from the MNIST dataset using PyTorch. We'll cover data loading and preprocessing, model definition, training, evaluation, and visualization of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Loading and Preprocessing\n",
    "\n",
    "We'll start by defining the transformations to apply to the images and loading the MNIST dataset.\n",
    "\n",
    "#### MNIST Dataset\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) dataset is a large database of handwritten digits that is commonly used for training various image processing systems. The dataset contains 70,000 images of handwritten digits (0-9) with 60,000 images in the training set and 10,000 images in the test set. Each image is a 28x28 grayscale image, meaning each pixel value ranges from 0 (black) to 255 (white). The dataset is well-suited for training and testing machine learning models in image recognition tasks, and it has become a standard benchmark for evaluating algorithms.\n",
    "\n",
    "\n",
    "\n",
    "#### torch.utils.data.DataLoader\n",
    "\n",
    "torch.utils.data.DataLoader is a utility in PyTorch that provides an efficient way to load and preprocess data. It allows for easy batching, shuffling, and parallel data loading using multiple workers. Key features include:\n",
    "\n",
    "1. `Batching`: Automatically groups data into batches.\n",
    "2. `Shuffling`: Randomizes the order of data samples, which is important for training neural networks.\n",
    "3. `Parallel Loading`: Utilizes multiple CPU cores to load data in parallel, reducing the time spent on data loading.\n",
    "\n",
    "DataLoader is essential for handling large datasets and ensuring efficient data feeding during the training and evaluation of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the images to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the pixel values with mean and std\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's visualize some sample images from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    ax.imshow(example_data[i][0], cmap='gray')\n",
    "    ax.set_title(f'Label: {example_targets[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Neural Network Model\n",
    "\n",
    "We will define a simple fully connected neural network with two layers.\n",
    "\n",
    "\n",
    "#### torch.nn Module\n",
    "\n",
    "The `torch.nn` module in PyTorch provides a wide range of classes and functions to help you build neural networks. It includes pre-defined layers, activation functions, loss functions, and utilities that can be used to define and train neural networks. Some key components include:\n",
    "\n",
    "1. `Layers`: nn.Linear, nn.Conv2d, nn.RNN, etc., to define various types of neural network layers.\n",
    "2. `Activations`: nn.ReLU, nn.Sigmoid, nn.Tanh, etc., to introduce non-linearities in the network.\n",
    "3. `Loss Functions`: nn.CrossEntropyLoss, nn.MSELoss, etc., to calculate the loss during training.\n",
    "4. `Container Modules`: nn.Sequential, nn.ModuleList, nn.ModuleDict, to organize and manage layers.\n",
    "\n",
    "Overall, torch.nn provides the building blocks for creating and training neural networks in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class FullyConnectedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "\n",
    "        # First Layer\n",
    "        self.layer_1 = nn.Linear(28*28, 512)\n",
    "        self.activation_1 = nn.ReLU() # ReLU activation\n",
    "\n",
    "        # Second layer\n",
    "        self.layer_2 = nn.Linear(512, 10)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # Calling the first layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.activation_1(x)\n",
    "\n",
    "        # Calling the second layer\n",
    "        x = self.layer_2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FullyConnectedNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define the Loss Function and Optimizer\n",
    "\n",
    "We'll use cross-entropy loss and stochastic gradient descent (SGD) optimizer.\n",
    "\n",
    "#### CrossEntropyLoss\n",
    "\n",
    "CrossEntropyLoss is a commonly used loss function for classification problems in neural networks. It combines LogSoftmax and NLLLoss (negative log-likelihood loss) in a single class. CrossEntropyLoss is particularly useful for multi-class classification tasks.\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize the loss function during the training of a machine learning model. Unlike traditional gradient descent, which computes the gradient of the loss function using the entire training dataset, SGD updates the model parameters using only a single training example or a small batch of examples at each iteration. This makes SGD more efficient and able to handle large datasets. Key features of SGD include:\n",
    "\n",
    "* Efficiency: Faster convergence on large datasets.\n",
    "* Stochastic Nature: Introduces noise in the parameter updates, which can help in escaping local minima.\n",
    "* Adjustable Learning Rate: Allows tuning of the learning rate to control the step size of parameter updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Training the Model\n",
    "\n",
    "Let's define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels).item() / len(labels)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    global running_loss\n",
    "    global running_acc\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy(outputs, labels)\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {i + 1}, Loss: {running_loss / 200:.4f}, Accuracy: {running_acc / 200:.4f}')\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "# Track training loss and accuracy\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(running_acc / len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluating the Model\n",
    "\n",
    "We'll define a function to evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test loop\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy(outputs, labels)\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Test the model\n",
    "all_preds, all_labels = test(model, device, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Visualizing Training Progress\n",
    "\n",
    "Let's plot the training loss and accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Confusion Matrix\n",
    "\n",
    "We'll plot the confusion matrix to see how well the model is performing across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(10)])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Visualizing Predictions\n",
    "\n",
    "Finally, let's visualize some sample predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images and predictions\n",
    "samples, labels = next(iter(test_loader))\n",
    "samples, labels = samples.to(device), labels.to(device)\n",
    "outputs = model(samples)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "samples = samples.cpu().numpy()\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {labels[i]}, Prediction: {preds[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some examples where the model made incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize incorrect predictions\n",
    "incorrect = [i for i in range(len(all_preds)) if all_preds[i] != all_labels[i]]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    idx = incorrect[i]\n",
    "    ax.imshow(test_loader.dataset[idx][0][0], cmap='gray')\n",
    "    ax.set_title(f'True: {all_labels[idx]}, Pred: {all_preds[idx]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusion\n",
    "\n",
    "In this notebook, we have built a simple fully connected neural network for classifying MNIST digits using PyTorch. We covered data loading, preprocessing, model definition, training, evaluation, and visualization of results. This provides a good foundation for understanding the process of developing neural network models for image classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
