{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumani\n",
    "# 5-8-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Key Concepts in Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Introduction\n",
    "Convolutional Neural Networks (CNNs) are a class of deep neural networks that have proven to be highly effective for various computer vision tasks, such as image classification, object detection, and segmentation. CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images.\n",
    "\n",
    "This notebook will cover the fundamental concepts that underpin CNNs, including convolution operations, padding, stride, feature maps, and filters. By understanding these basics, students will be well-equipped to delve into more complex topics and applications of CNNs.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Convolution\n",
    "Convolution is the cornerstone of CNNs. It involves sliding a filter (also known as a kernel) across the input image and computing the dot product between the filter and a portion of the input at each position. This process extracts features such as edges, textures, and patterns from the input image. Convolution reduces the spatial dimensions of the input while retaining the essential features needed for the learning process.\n",
    "\n",
    "### Filters (Kernels)\n",
    "Filters are small matrices that slide over the input image to perform the convolution operation. Each filter is trained to detect a specific feature, such as an edge, corner, or texture. The depth of a filter matches the number of channels in the input image (e.g., 3 channels for RGB images). The values in the filter are learned during the training process to optimize the feature extraction.\n",
    "\n",
    "### Feature Maps\n",
    "Feature maps are the outputs of the convolutional layers. They represent the presence of specific features detected by the filters at various spatial locations in the input image. Each filter produces a separate feature map, and stacking these maps forms the complete set of features learned by the CNN.\n",
    "\n",
    "### Padding\n",
    "Padding refers to adding extra pixels around the border of the input image. This is done to control the spatial dimensions of the output feature map. There are two main types of padding:\n",
    "- **Valid Padding**: No padding is added, and the filter is only applied to valid positions within the input. This results in an output that is smaller than the input.\n",
    "- **Same Padding**: Padding is added to ensure that the output feature map has the same spatial dimensions as the input. This is useful for preserving the spatial resolution.\n",
    "\n",
    "### Stride\n",
    "Stride is the step size by which the filter moves across the input image. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time. Stride affects the spatial dimensions of the output feature map. Larger strides result in smaller output dimensions and faster computations, but can also miss finer details in the input.\n",
    "\n",
    "### Pooling Layers\n",
    "Pooling layers are used to reduce the spatial dimensions of the feature maps, making the computation more efficient and reducing the risk of overfitting. The most common types of pooling are:\n",
    "- **Max Pooling**: Takes the maximum value in each patch of the feature map.\n",
    "- **Average Pooling**: Takes the average value in each patch of the feature map.\n",
    "\n",
    "### Activation Functions\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn more complex patterns. The most common activation function in CNNs is the Rectified Linear Unit (ReLU), which replaces all negative values with zero.\n",
    "\n",
    "### Regularization Techniques\n",
    "Regularization techniques are used to prevent overfitting and improve the generalization of the model. Common techniques include:\n",
    "- **Dropout**: Randomly sets a fraction of the input units to zero during training.\n",
    "- **Batch Normalization**: Normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n",
    "\n",
    "### Advanced Concepts\n",
    "- **Transfer Learning**: Using pre-trained models on new tasks to leverage learned features.\n",
    "- **Backpropagation in CNNs**: Understanding how gradients are calculated and weights are updated during training.\n",
    "- **Data Augmentation**: Techniques to artificially increase the size of the training dataset by creating modified versions of images.\n",
    "\n",
    "By the end of this notebook, you will have a solid understanding of these fundamental concepts and how they are implemented in PyTorch. This knowledge will serve as a foundation for building and training your own CNNs for various computer vision tasks.\n",
    "\n",
    "Let's dive in and explore each concept in detail with practical examples and code implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolution\n",
    "\n",
    "Convolution is a mathematical operation used in deep learning, especially in Convolutional Neural Networks (CNNs), to extract features from input data. It involves sliding a filter (also called a kernel) over the input data and computing the dot product at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple input tensor (3x3)\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                             [4.0, 5.0, 6.0],\n",
    "                             [7.0, 8.0, 9.0]])\n",
    "\n",
    "# Define a simple filter (2x2)\n",
    "filter_tensor = torch.tensor([[1.0, 0.0],\n",
    "                              [0.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform convolution without padding and stride of 1\n",
    "conv_output = F.conv2d(input_tensor.unsqueeze(0).unsqueeze(0), filter_tensor.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the result\n",
    "print(\"Convolution Output (no padding, stride 1):\\n\", conv_output)\n",
    "\n",
    "# The filter slides over the input tensor without any padding, resulting in a smaller output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Maps and Filters\n",
    "\n",
    "Feature maps are the output of applying filters (kernels) to the input data. Multiple filters can be used to extract different features from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input tensor (1x1x3x3)\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0],\n",
    "                               [4.0, 5.0, 6.0],\n",
    "                               [7.0, 8.0, 9.0]]]])\n",
    "\n",
    "# Define multiple filters\n",
    "filter_tensor = torch.tensor([[[[1.0, 0.0],\n",
    "                                [0.0, -1.0]]],\n",
    "                              [[[0.0, 1.0],\n",
    "                                [-1.0, 0.0]]]])\n",
    "\n",
    "# Perform convolution with multiple filters\n",
    "conv_output_multiple_filters = F.conv2d(input_tensor, filter_tensor)\n",
    "print(\"Convolution Output with Multiple Filters (Feature Maps):\\n\", conv_output_multiple_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Padding\n",
    "\n",
    "Padding is the addition of extra pixels (usually zeros) to the border of the input data. It helps in maintaining the spatial dimensions of the output after convolution and ensures that edge information is not lost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform convolution with padding of 1 and stride of 1\n",
    "conv_output_padded = F.conv2d(input_tensor, filter_tensor, padding=1)\n",
    "\n",
    "# Output the result\n",
    "print(\"Convolution Output (padding 1, stride 1):\\n\", conv_output_padded)\n",
    "\n",
    "# Padding of 1 is added around the input tensor, allowing the filter to slide over the borders, \n",
    "# resulting in an output tensor of the same size as the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Valid Padding and Same Padding\n",
    "\n",
    "Padding is the addition of extra pixels around the input data. Valid padding means no padding is added, while same padding ensures the output has the same dimensions as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input tensor (1x1x3x3)\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0],\n",
    "                               [4.0, 5.0, 6.0],\n",
    "                               [7.0, 8.0, 9.0]]]])\n",
    "\n",
    "# Define a simple filter (2x2)\n",
    "filter_tensor = torch.tensor([[[[1.0, 0.0],\n",
    "                                [0.0, -1.0]]]])\n",
    "\n",
    "# Convolution with valid padding (no padding)\n",
    "conv_output_valid = F.conv2d(input_tensor, filter_tensor, padding=0)\n",
    "print(\"Convolution Output (Valid Padding):\\n\", conv_output_valid)\n",
    "\n",
    "# Convolution with same padding (padding=1)\n",
    "conv_output_same = F.conv2d(input_tensor, filter_tensor, padding=1)\n",
    "print(\"Convolution Output (Same Padding):\\n\", conv_output_same)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stride\n",
    "Stride is the number of pixels by which the filter moves over the input data. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means the filter moves two pixels at a time. Larger strides result in smaller output dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform convolution with padding of 1 and stride of 2\n",
    "conv_output_stride = F.conv2d(input_tensor, filter_tensor, padding=1, stride=2)\n",
    "\n",
    "# Output the result\n",
    "print(\"Convolution Output (padding 1, stride 2):\\n\", conv_output_stride)\n",
    "\n",
    "# The filter moves with larger steps (stride of 2), resulting in a smaller output tensor even with padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Horizontal and Vertical Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input tensor (1x1x4x4)\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0, 4.0],\n",
    "                               [5.0, 6.0, 7.0, 8.0],\n",
    "                               [9.0, 10.0, 11.0, 12.0],\n",
    "                               [13.0, 14.0, 15.0, 16.0]]]])\n",
    "\n",
    "# Define a simple filter (2x2)\n",
    "filter_tensor = torch.tensor([[[[1.0, 0.0],\n",
    "                                [0.0, -1.0]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution with horizontal stride of 2 and vertical stride of 1\n",
    "conv_output_h_stride = F.conv2d(input_tensor, filter_tensor, stride=(1, 2))\n",
    "print(\"Convolution Output (horizontal stride 2, vertical stride 1):\\n\", conv_output_h_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution with horizontal stride of 1 and vertical stride of 2\n",
    "conv_output_v_stride = F.conv2d(input_tensor, filter_tensor, stride=(2, 1))\n",
    "print(\"Convolution Output (horizontal stride 1, vertical stride 2):\\n\", conv_output_v_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input tensor\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0],\n",
    "                               [4.0, 5.0, 6.0],\n",
    "                               [7.0, 8.0, 9.0]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling\n",
    "max_pool_output = F.max_pool2d(input_tensor, kernel_size=2, stride=1)\n",
    "print(\"Max Pooling Output:\\n\", max_pool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pooling\n",
    "avg_pool_output = F.avg_pool2d(input_tensor, kernel_size=2, stride=1)\n",
    "print(\"Average Pooling Output:\\n\", avg_pool_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Dropout Works?\n",
    "1. Randomly zeroing neurons: During training, dropout randomly sets a fraction of neurons (specified by the p parameter) to zero\n",
    "2. Different neurons dropped each time: The neurons dropped out vary for each forward pass through the network.\n",
    "3. Scaling: To compensate for the dropped neurons, the remaining activations are scaled by a factor of 1 / (1 - p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input tensor\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 3.0],\n",
    "                               [4.0, 5.0, 6.0],\n",
    "                               [7.0, 8.0, 9.0]]]])\n",
    "\n",
    "# Dropout (during training)\n",
    "dropout_layer = torch.nn.Dropout(p=0.5)\n",
    "dropout_output = dropout_layer(input_tensor)\n",
    "\n",
    "print(\"Input tensor (training mode):\\n\", input_tensor)\n",
    "print(\"Dropout Output (training mode):\\n\", dropout_output)\n",
    "\n",
    "# Note that, the non-zero numbers are doubled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization\n",
    "batch_norm_layer = torch.nn.BatchNorm2d(1)\n",
    "batch_norm_output = batch_norm_layer(input_tensor)\n",
    "print(\"Batch Normalization Output:\\n\", batch_norm_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
