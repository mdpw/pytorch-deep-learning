{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumani\n",
    "# 29-7-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a continuation of the previous Jupyter notebook [02_fully_connected_mnist.ipynb](./02_fully_connected_mnist.ipynb), where we built and trained a fully connected neural network to classify handwritten digits from the MNIST dataset using PyTorch. Having understood the fundamental steps involved in model development, this notebook provides a series of incremental exercises designed to deepen your understanding of various aspects of neural network training and optimization.\n",
    "\n",
    "Each exercise introduces a small modification to the existing model or training process. These modifications will help you explore the effects of different model architectures, activation functions, optimizers, learning rates, regularization techniques, data augmentation, loss functions, and evaluation metrics. By completing these exercises, you will gain hands-on experience in tuning neural networks and improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use This Notebook\n",
    "1. **Review the Previous Notebook**: Ensure you are familiar with the steps and code in the previous notebook, as this notebook builds upon that foundation.\n",
    "2. **Complete the Exercises**: Work through each exercise one by one. Make the specified changes to the model or training process and run the cells to see the effects.\n",
    "3. **Document Your Observations**: For each exercise, take note of how the changes impact the model's training and evaluation metrics. Provide explanations for any improvements or deteriorations in performance.\n",
    "4. **Experiment and Explore**: Feel free to experiment further by combining different techniques or exploring additional modifications beyond the provided exercises.\n",
    "\n",
    "By the end of this notebook, you will have a deeper understanding of how various factors influence the training and performance of neural networks, equipping you with the knowledge and skills to effectively tune and optimize models for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Modify the Model Architecture\n",
    "\n",
    "    Add a Hidden Layer: Add an additional hidden layer with 256 neurons between the existing layers. Re-train the model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Change the Activation Function\n",
    "\n",
    "    Use Different Activation Functions: Replace the ReLU activation function with other activation functions such as Sigmoid or Tanh. Observe the impact on training and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Change the Optimizer\n",
    "\n",
    "    Use Adam Optimizer: Replace the SGD optimizer with the Adam optimizer. Compare the training speed and final accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Adjust the Learning Rate\n",
    "\n",
    "    Experiment with Learning Rates: Try different learning rates (e.g., 0.1, 0.001, 0.0001) with the SGD and Adam optimizers. Record the effects on the model's training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Implement Dropout\n",
    "\n",
    "    Add Dropout Layers: Introduce dropout layers with a dropout probability of 0.5 to the network. Check if the model's performance improves by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Batch Normalization\n",
    "\n",
    "    Add Batch Normalization: Incorporate batch normalization layers into the network and observe any changes in training stability and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Data Augmentation\n",
    "\n",
    "    Apply Data Augmentation: Implement data augmentation techniques like random rotations, shifts, and flips to the training dataset. Evaluate the model's robustness to these transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Learning Rate Scheduling\n",
    "\n",
    "    Implement Learning Rate Scheduling: Use a learning rate scheduler to decrease the learning rate during training. Compare the training process and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9: Change the Loss Function\n",
    "\n",
    "    Use Different Loss Functions: Experiment with different loss functions such as Mean Squared Error (MSE) and observe how the choice of loss function affects model training and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: Evaluate with Different Metrics\n",
    "\n",
    "    Implement Precision, Recall, and F1-Score: Extend the evaluation metrics to include precision, recall, and F1-score. Analyze the model's performance using these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "These exercises will help you to understand the impact of different neural network architectures, optimization techniques, and evaluation metrics on the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
