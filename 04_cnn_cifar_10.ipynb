{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumani\n",
    "# 5-8-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training a CNN model on CIFAR-10 Dataset Using PyTorch\n",
    "\n",
    "This notebook demonstrates how to build a Convolutional Neural Network (CNN) using PyTorch to classify images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset is a widely used dataset for training machine learning and computer vision algorithms. It was created by the Canadian Institute For Advanced Research (CIFAR). The CIFAR-10 dataset is often used to benchmark the performance of new algorithms in the field of image classification. It provides a challenging classification task where each image must be correctly labeled with one of the 10 classes.\n",
    "\n",
    "Here are some key points about the dataset:\n",
    "\n",
    "* Number of Images: The dataset contains 60,000 color images, each of size 32x32 pixels.\n",
    "* Classes: There are 10 different classes in the CIFAR-10 dataset. Each class represents a distinct object category: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck.\n",
    "\n",
    "* Training and Testing Split: The dataset is divided into two parts:\n",
    "    * 50,000 training images\n",
    "    * 10,000 test images\n",
    "\n",
    "Image Characteristics: All images in the dataset are RGB (color) images, and each image has dimensions of 32x32 pixels. This small size makes the dataset suitable for experimenting with various image recognition algorithms without requiring extensive computational resources.\n",
    "\n",
    "Normalization: Before training a model on the CIFAR-10 dataset, it is common practice to normalize the images. Normalization helps in speeding up the training process and achieving better performance. Typically, the images are normalized to have a mean and standard deviation of 0.5 for each of the RGB channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We start by importing the necessary libraries. PyTorch is a popular deep learning framework, and torchvision provides utilities for working with image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "We load the CIFAR-10 dataset and apply transformations to the images. These transformations include random horizontal flipping, random cropping, and normalization. Normalization scales the pixel values to a range that is suitable for training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformations for training and testing datasets\n",
    "\n",
    "In computer vision, it is a standard practice to transform the dataset. These transformations help in augmenting the training data and standardizing the input, which can lead to better model performance and generalization.\n",
    "\n",
    "* *transforms.Compose*: transforms.Compose is a method provided by the torchvision.transforms module. It allows you to chain multiple image transformations together. This means that each transformation is applied sequentially to the images.\n",
    "\n",
    "* *transforms.RandomHorizontalFlip()*: This transformation randomly flips the image horizontally with a probability of 0.5. Horizontal flipping is a common data augmentation technique that helps to make the model invariant to the horizontal orientation of objects in the image. This can improve the model's generalization to unseen data.\n",
    "\n",
    "* *transforms.RandomCrop(32, padding=4)*: This transformation crops the image to a size of 32x32 pixels. The padding=4 argument specifies that 4 pixels of padding (with zeros) should be added to each side of the image before cropping. The padded image will have dimensions of 40x40 pixels, from which a 32x32 patch is randomly cropped. This data augmentation technique helps the model learn to be robust to slight shifts and translations of the objects within the images.\n",
    "\n",
    "* *transforms.ToTensor()*: This transformation converts the image from a PIL image or a NumPy array to a PyTorch tensor. This conversion is necessary because PyTorch models expect input data in the form of tensors. The image data is also scaled from a range of [0, 255] to [0, 1].\n",
    "\n",
    "* *transforms.Normalize()*: This transformation normalizes the image tensor by subtracting the mean and dividing by the standard deviation for each color channel (RGB). Normalization helps to standardize the pixel values and speeds up the convergence of the training process. It also ensures that each feature (pixel value) has a similar scale, which can improve the performance of the neural network. The mean and standard deviation values used here are specific to the CIFAR-10 dataset:\n",
    "    * Mean: (0.4914, 0.4822, 0.4465)\n",
    "    * Standard Deviation: (0.2023, 0.1994, 0.2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and testing datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN Architecture\n",
    "\n",
    "Here, we define the architecture of our CNN. The network consists of three convolutional layers, each followed by a ReLU activation function and max-pooling layer. The convolutional layers detect features in the input images, while the pooling layers reduce the spatial dimensions. Fully connected layers are used at the end to classify the extracted features into one of the 10 classes.\n",
    "\n",
    "Explanation:\n",
    "* Conv2d: This layer creates a convolutional kernel that is convolved with the input layer over a single spatial dimension to produce a tensor of outputs.\n",
    "* MaxPool2d: Applies a 2D max pooling operation over an input signal composed of several input planes.\n",
    "* Linear: Applies a linear transformation to the incoming data.\n",
    "* Dropout: Randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimizer\n",
    "\n",
    "The loss function measures how well the network's predictions match the actual labels. We use CrossEntropyLoss, which is suitable for classification tasks. The optimizer updates the network's weights based on the gradients computed during backpropagation. We use the Adam optimizer, which is a popular choice due to its adaptive learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "\n",
    "We train the network for a specified number of epochs. In each epoch, we pass the entire training dataset through the network, compute the loss, and update the weights. We also evaluate the network on the test dataset after each epoch to monitor its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Print every 100th batch\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    print(f'Full Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss = test_loss / len(testloader)\n",
    "    test_acc = 100. * correct / total\n",
    "    test_loss_history.append(test_loss)\n",
    "    test_acc_history.append(test_acc)\n",
    "\n",
    "    print(f'Full Epoch {epoch+1}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training and Testing Loss\n",
    "\n",
    "We plot the training and testing loss to visualize the model's learning process over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(test_loss_history, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training and Testing Accuracy\n",
    "\n",
    "We plot the training and testing accuracy to see how well the model is performing on both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(test_acc_history, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We evaluate the trained model on the test dataset to get the overall accuracy and accuracy for each class.\n",
    "\n",
    "\n",
    "Explanation:\n",
    "* Overall Accuracy: Measures how many test images are correctly classified by the model.\n",
    "* Class-wise Accuracy: Provides insight into how well the model performs on each individual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Accuracy of {classes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Some Test Images and Predictions\n",
    "\n",
    "We display some test images along with their ground truth labels and the model's predictions to visualize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]}' for j in range(4)))\n",
    "\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]}' for j in range(4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
